#!/bin/bash
# Quick Start Guide - Testing & Validation Framework
# This guide shows how to run all tests and verify the system

echo "=================================================="
echo "TF-IDF Compliance Drift - Testing Quick Start"
echo "=================================================="
echo ""

# Step 1: Install dependencies
echo "STEP 1: Installing Development Dependencies"
echo "─────────────────────────────────────────────"
echo "Command: pip install -r requirements-dev.txt"
echo ""
echo "This installs:"
echo "  • pytest (testing framework)"
echo "  • pytest-cov (code coverage)"
echo "  • pytest-xdist (parallel execution)"
echo "  • black, isort, flake8 (code quality)"
echo "  • mypy (type checking)"
echo ""

# Step 2: Verify syntax
echo "STEP 2: Verify Python Syntax"
echo "─────────────────────────────"
echo "Command: python -m py_compile dashboard/app.py"
echo ""
echo "Expected Output:"
echo "  (no errors = ✅ valid)"
echo ""

# Step 3: Run all tests
echo "STEP 3: Run All Tests"
echo "────────────────────"
echo "Command: pytest tests/ -v"
echo ""
echo "This will run:"
echo "  • 18 TF-IDF math tests"
echo "  • 25 preprocessing tests"
echo "  • 15 classification tests"
echo "  • 20 edge case tests"
echo "  • 22 input validation tests"
echo "  ────────────────────────"
echo "  • 97 TOTAL TESTS"
echo ""

# Step 4: Run with coverage
echo "STEP 4: Run Tests with Coverage Report"
echo "──────────────────────────────────────"
echo "Command: pytest tests/ --cov=dashboard --cov-report=html"
echo ""
echo "This generates:"
echo "  • Terminal coverage report"
echo "  • htmlcov/index.html (detailed HTML report)"
echo ""

# Step 5: Run specific test module
echo "STEP 5: Run Specific Test Module"
echo "───────────────────────────────"
echo "Examples:"
echo "  pytest tests/test_tfidf_math.py -v           # TF-IDF tests"
echo "  pytest tests/test_preprocessing.py -v        # Preprocessing tests"
echo "  pytest tests/test_classification.py -v       # Classification tests"
echo "  pytest tests/test_edge_cases.py -v           # Edge case tests"
echo "  pytest tests/test_input_validation.py -v     # Validation tests"
echo ""

# Step 6: Run specific test class
echo "STEP 6: Run Specific Test Class"
echo "──────────────────────────────"
echo "Examples:"
echo "  pytest tests/test_tfidf_math.py::TestTermFrequency -v"
echo "  pytest tests/test_preprocessing.py::TestBasicPreprocessing -v"
echo "  pytest tests/test_classification.py::TestClassificationBasic -v"
echo ""

# Step 7: Run in parallel
echo "STEP 7: Run Tests in Parallel (Faster)"
echo "─────────────────────────────────────"
echo "Command: pytest tests/ -n auto"
echo ""
echo "Benefits:"
echo "  • Faster execution (uses all CPU cores)"
echo "  • Still shows verbose output with -v"
echo ""

# Step 8: Run only specific markers
echo "STEP 8: Run Tests by Marker"
echo "──────────────────────────"
echo "Examples:"
echo "  pytest tests/ -m edge_case -v    # Only edge case tests"
echo "  pytest tests/ -m unit -v         # Only unit tests"
echo ""

# Step 9: Test the dashboard app
echo "STEP 9: Run the Dashboard Application"
echo "────────────────────────────────────"
echo "Command: streamlit run dashboard/app.py"
echo ""
echo "Then:"
echo "  1. Open browser to http://localhost:8501"
echo "  2. Navigate to file upload section"
echo "  3. Try uploading test files to verify validation"
echo ""

# Step 10: Check documentation
echo "STEP 10: Review Documentation"
echo "────────────────────────────"
echo "Read these files for more details:"
echo "  • SECURITY_VALIDATION.md (comprehensive guide)"
echo "  • SECURITY_QUICK_REFERENCE.md (quick lookup)"
echo "  • IMPLEMENTATION_SUMMARY.md (full details)"
echo "  • SESSION_SUMMARY.md (this session)"
echo ""

echo "=================================================="
echo "QUICK TEST COMMANDS"
echo "=================================================="
echo ""

echo "Run all tests:"
echo "  pytest tests/ -v"
echo ""

echo "Run with coverage:"
echo "  pytest tests/ --cov=dashboard --cov-report=html"
echo ""

echo "Run specific test file:"
echo "  pytest tests/test_tfidf_math.py -v"
echo ""

echo "Run in parallel (faster):"
echo "  pytest tests/ -n auto"
echo ""

echo "Run dashboard:"
echo "  streamlit run dashboard/app.py"
echo ""

echo "=================================================="
echo "TEST COVERAGE AREAS"
echo "=================================================="
echo ""

echo "TF-IDF Mathematics (18 tests)"
echo "  ✓ Term frequency computation"
echo "  ✓ IDF computation (4 variants)"
echo "  ✓ Manual vs sklearn comparison"
echo "  ✓ Edge cases (no NaN/Inf)"
echo ""

echo "Text Preprocessing (25 tests)"
echo "  ✓ Lowercase, punctuation removal"
echo "  ✓ Stopword filtering"
echo "  ✓ Number handling"
echo "  ✓ Lemmatization"
echo "  ✓ Unicode and special characters"
echo "  ✓ Edge cases (long text, URLs, emails)"
echo ""

echo "Classification (15 tests)"
echo "  ✓ Sufficient data handling"
echo "  ✓ Insufficient data rejection"
echo "  ✓ Imbalanced data filtering"
echo "  ✓ Model accuracy validation"
echo "  ✓ Confusion matrices and reports"
echo ""

echo "Edge Cases (20 tests)"
echo "  ✓ Parameter validation (min_df, max_df)"
echo "  ✓ Empty documents"
echo "  ✓ Single document"
echo "  ✓ Category constraints"
echo "  ✓ Special distributions"
echo "  ✓ Data type validation"
echo ""

echo "Input Validation (22 tests)"
echo "  ✓ File size limits"
echo "  ✓ File extension validation"
echo "  ✓ PDF magic bytes check"
echo "  ✓ Text encoding validation"
echo "  ✓ Error message clarity"
echo "  ✓ Security scenarios"
echo "  ✓ Metrics tracking"
echo ""

echo "=================================================="
echo "EXPECTED TEST OUTPUT"
echo "=================================================="
echo ""

echo "When you run 'pytest tests/ -v', you should see:"
echo ""
echo "  test_tfidf_math.py::TestTermFrequency::test_compute_term_frequency_basic PASSED"
echo "  test_tfidf_math.py::TestTermFrequency::test_compute_term_frequency_empty PASSED"
echo "  ... (more tests)"
echo ""
echo "  ====== 97 passed in X.XXs ======"
echo ""

echo "=================================================="
echo "TROUBLESHOOTING"
echo "=================================================="
echo ""

echo "If tests fail:"
echo "  1. Check Python version: python --version"
echo "  2. Check dependencies: pip list | grep pytest"
echo "  3. Reinstall dev dependencies: pip install -r requirements-dev.txt --force-reinstall"
echo "  4. Check test file syntax: python -m py_compile tests/*.py"
echo ""

echo "If imports fail:"
echo "  1. Verify src is in path"
echo "  2. Check conftest.py adds src to sys.path"
echo "  3. Run from project root: cd tfidf-compliance-drift"
echo ""

echo "If validation tests fail:"
echo "  1. Check dashboard/app.py has validate_input_file function (line 991+)"
echo "  2. Verify CONFIG dataclass has MAX_FILE_SIZE_MB (line 82)"
echo "  3. Run syntax check: python -m py_compile dashboard/app.py"
echo ""

echo "=================================================="
echo "NEXT STEPS"
echo "=================================================="
echo ""

echo "1. INSTALL DEPENDENCIES:"
echo "   pip install -r requirements-dev.txt"
echo ""

echo "2. RUN TESTS:"
echo "   pytest tests/ -v"
echo ""

echo "3. CHECK COVERAGE:"
echo "   pytest tests/ --cov=dashboard --cov-report=html"
echo "   open htmlcov/index.html"
echo ""

echo "4. TEST THE APP:"
echo "   streamlit run dashboard/app.py"
echo ""

echo "5. READ DOCUMENTATION:"
echo "   SECURITY_VALIDATION.md - Complete guide"
echo "   SECURITY_QUICK_REFERENCE.md - Quick lookup"
echo ""

echo "=================================================="
echo "Done! Happy testing!"
echo "=================================================="
