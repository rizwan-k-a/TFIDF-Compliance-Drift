{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da93162e",
   "metadata": {},
   "source": [
    "# TF‑IDF & Cosine Similarity — Derivations\n",
    "\n",
    "This notebook documents the mathematical formulas and implements reproducible code snippets for TF‑IDF based compliance drift detection.\n",
    "\n",
    "Outline:\n",
    "1. Environment & dependencies\n",
    "2. Project layout & path variables\n",
    "3. Load metadata and documents\n",
    "4. Text preprocessing\n",
    "5. TF‑IDF vectorization\n",
    "6. Cosine similarity calculations\n",
    "7. Drift computation across versions\n",
    "8. Threshold calibration & statistical checks\n",
    "9. Alerting & export of results\n",
    "10. Streamlit dashboard integration\n",
    "11. Unit tests for core modules\n",
    "12. Reproduce experiments & notebooks\n",
    "13. Run & debug in VS Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 — Environment & dependencies\n",
    "\n",
    "# Install packages (run in your environment once)\n",
    "!pip install -r ../requirements.txt\n",
    "\n",
    "# Verify python and key package versions\n",
    "!python --version\n",
    "python -c \"import sklearn, pandas, streamlit; import sys; print('sklearn', sklearn.__version__); print('pandas', pandas.__version__); print('streamlit', streamlit.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fce33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 — Project layout & path variables\n",
    "from pathlib import Path\n",
    "ROOT = Path('..').resolve()\n",
    "DATA = ROOT / 'data'\n",
    "SRC = ROOT / 'src'\n",
    "NOTEBOOKS = ROOT / 'notebooks'\n",
    "RESULTS = ROOT / 'results'\n",
    "DASHBOARD = ROOT / 'dashboard'\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('DATA:', DATA)\n",
    "\n",
    "# helper: list files\n",
    "print('\\nreference files:')\n",
    "for p in sorted((DATA / 'reference').glob('*.txt')):\n",
    "    print('-', p.name)\n",
    "\n",
    "print('\\ninternal files:')\n",
    "for p in sorted((DATA / 'internal').glob('*.txt')):\n",
    "    print('-', p.name)\n",
    "\n",
    "METADATA_CSV = DATA / 'metadata.csv'\n",
    "print('\\nmetadata:', METADATA_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 — Load metadata and documents\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "meta = pd.read_csv(METADATA_CSV)\n",
    "meta.head()\n",
    "\n",
    "# Map doc_id -> file path (assumes filenames in metadata)\n",
    "meta['path'] = meta['filename'].apply(lambda fn: DATA / ('reference' if fn.startswith('guideline') else 'internal') / fn)\n",
    "\n",
    "# Load text into dataframe\n",
    "def load_texts(df):\n",
    "    texts = []\n",
    "    for _, row in df.iterrows():\n",
    "        p = row['path']\n",
    "        try:\n",
    "            texts.append(p.read_text(encoding='utf-8'))\n",
    "        except Exception as e:\n",
    "            texts.append('')\n",
    "    df['text'] = texts\n",
    "    return df\n",
    "\n",
    "meta = load_texts(meta)\n",
    "meta[['doc_id','version','date','path','text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 — Text preprocessing (using src/preprocess.py)\n",
    "from src.preprocess import clean_text\n",
    "\n",
    "# simple tokenization example\n",
    "sample = meta['text'].iloc[0] if len(meta)>0 else 'Example text, with punctuation.'\n",
    "cleaned = clean_text(sample)\n",
    "print('Original:', sample[:200])\n",
    "print('\\nCleaned:', cleaned[:200])\n",
    "\n",
    "# simple tokenization and unit test\n",
    "tokens = cleaned.split()\n",
    "assert isinstance(tokens, list)\n",
    "print('\\nTokens count:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9addc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 — TF‑IDF vectorization (src/vectorize.py)\n",
    "from src.vectorize import fit_vectorizer, transform_documents\n",
    "\n",
    "# prepare corpora: reference vs internal\n",
    "refs = meta[meta['path'].str.contains('reference')]['text'].tolist()\n",
    "internals = meta[~meta['path'].str.contains('reference')]['text'].tolist()\n",
    "\n",
    "vect, X_refs = fit_vectorizer(refs, max_features=2000)\n",
    "X_internals = transform_documents(vect, internals)\n",
    "\n",
    "print('Reference matrix shape:', X_refs.shape)\n",
    "print('Internal matrix shape:', X_internals.shape)\n",
    "\n",
    "# TF-IDF formula\n",
    "display_markdown = \"\"\"\n",
    "The TF‑IDF weighting used is:\n",
    "\n",
    "$$tfidf_{t,d} = tf_{t,d} \\cdot \\log\\frac{N}{df_t}$$\n",
    "\n",
    "where $tf_{t,d}$ is term frequency and $df_t$ is document frequency.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(display_markdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879443f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6 — Cosine similarity calculations (src/similarity.py)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# compute pairwise similarities between internals and refs\n",
    "sim_matrix = cosine_similarity(X_internals, X_refs)\n",
    "print('Similarity matrix shape (internals x refs):', sim_matrix.shape)\n",
    "\n",
    "# show top-1 match for each internal doc\n",
    "top_indices = np.argmax(sim_matrix, axis=1)\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f'Internal doc {i} best matches reference {idx} with sim={sim_matrix[i, idx]:.3f}')\n",
    "\n",
    "# formula\n",
    "Markdown('''$$\\cos(\\mathbf{a},\\mathbf{b})=\\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\|\\mathbf{a}\\|\\,\\|\\mathbf{b}\\|}$$''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c98e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7 — Drift computation across versions (src/drift.py)\n",
    "import numpy as np\n",
    "from src.drift import compute_drift\n",
    "\n",
    "# compute drift per internal doc as 1 - max similarity to refs\n",
    "drift_scores = compute_drift(sim_matrix)\n",
    "print('Drift scores:', drift_scores)\n",
    "\n",
    "# If metadata contains versions, we can aggregate per doc_id across versions\n",
    "# Example: construct a simple dataframe tying internals to doc_ids (if present)\n",
    "internal_meta = meta[~meta['path'].str.contains('reference')].reset_index(drop=True)\n",
    "internal_meta['drift_score'] = drift_scores\n",
    "internal_meta[['doc_id','version','date','drift_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8 — Threshold calibration & statistical checks\n",
    "import numpy as np\n",
    "\n",
    "# percentile-based threshold example\n",
    "threshold_95 = np.percentile(internal_meta['drift_score'].fillna(0), 95)\n",
    "print('95th percentile threshold:', threshold_95)\n",
    "\n",
    "# simple z-score calibration\n",
    "mu = internal_meta['drift_score'].mean()\n",
    "sigma = internal_meta['drift_score'].std(ddof=0)\n",
    "internal_meta['zscore'] = (internal_meta['drift_score'] - mu) / (sigma + 1e-9)\n",
    "print('\\nmean, std:', mu, sigma)\n",
    "\n",
    "# bootstrap example (small, illustrative only)\n",
    "import random\n",
    "def bootstrap_percentile(scores, n=1000):\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        s = [random.choice(list(scores)) for _ in range(len(scores))]\n",
    "        samples.append(np.percentile(s, 95))\n",
    "    return np.percentile(samples, [2.5, 97.5])\n",
    "\n",
    "if len(internal_meta)>0:\n",
    "    ci = bootstrap_percentile(internal_meta['drift_score'].fillna(0).values, n=200)\n",
    "    print('Bootstrap 95% CI for 95th percentile:', ci)\n",
    "else:\n",
    "    print('Not enough data for bootstrap example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738af1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9 — Alerting & export of results (src/alerts.py)\n",
    "from src.alerts import make_alerts\n",
    "\n",
    "threshold = float(threshold_95) if not np.isnan(threshold_95) else 0.4\n",
    "alerts_df = make_alerts(internal_meta['doc_id'].tolist(), internal_meta['drift_score'].fillna(0).values, threshold)\n",
    "\n",
    "# add extra metadata columns\n",
    "alerts_df['version'] = internal_meta['version']\n",
    "alerts_df['date'] = internal_meta['date']\n",
    "\n",
    "# save results\n",
    "RESULTS_PATH = RESULTS / 'drift_alerts.csv'\n",
    "SIM_PATH = RESULTS / 'similarity_scores.csv'\n",
    "alerts_df.to_csv(RESULTS_PATH, index=False)\n",
    "print('Wrote', RESULTS_PATH)\n",
    "\n",
    "# simplified similarity export: top match per internal doc\n",
    "sim_rows = []\n",
    "for i, doc in internal_meta.iterrows():\n",
    "    ref_idx = int(top_indices[i]) if i < len(top_indices) else -1\n",
    "    sim_rows.append({'doc_id': doc['doc_id'], 'ref_id': ref_idx, 'similarity': float(sim_matrix[i, ref_idx]) if ref_idx>=0 else 0.0})\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(sim_rows).to_csv(SIM_PATH, index=False)\n",
    "print('Wrote', SIM_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10 — Streamlit dashboard integration (dashboard/app.py)\n",
    "\n",
    "# Run with:\n",
    "# streamlit run ../dashboard/app.py\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown('Streamlit app uses `results/drift_alerts.csv` and `results/similarity_scores.csv`.')\n",
    "\n",
    "# Quick preview of results\n",
    "try:\n",
    "    df_alerts = pd.read_csv(RESULTS_PATH)\n",
    "    df_alerts.head()\n",
    "except Exception as e:\n",
    "    print('No results yet — run the export cell to generate CSVs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee81f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11 — Unit tests for core modules (examples)\n",
    "\n",
    "# Example pytest-style tests (place under tests/test_core.py)\n",
    "example_tests = '''\n",
    "import pytest\n",
    "from src.preprocess import clean_text\n",
    "from src.vectorize import fit_vectorizer\n",
    "\n",
    "\n",
    "def test_clean_text():\n",
    "    s = 'Hello, WORLD!!!'\n",
    "    assert clean_text(s) == 'hello world'\n",
    "\n",
    "\n",
    "def test_vectorizer_small():\n",
    "    docs = ['a b c', 'a b', 'b c']\n",
    "    vect, X = fit_vectorizer(docs, max_features=10)\n",
    "    assert X.shape[0] == 3\n",
    "'''\n",
    "\n",
    "print(example_tests)\n",
    "\n",
    "# Run tests: `pytest -q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12 — Reproduce experiments & notebooks\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Example: sweep max_features\n",
    "params = [500, 1000, 2000]\n",
    "results = []\n",
    "for mf in params:\n",
    "    vect, Xr = fit_vectorizer(refs, max_features=mf)\n",
    "    Xi = transform_documents(vect, internals)\n",
    "    sim = cosine_similarity(Xi, Xr)\n",
    "    drift = 1 - sim.max(axis=1)\n",
    "    results.append({'max_features': mf, 'mean_drift': float(drift.mean())})\n",
    "\n",
    "import pandas as pd\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75d782",
   "metadata": {},
   "source": [
    "# Section 13 — Run & debug in VS Code\n",
    "\n",
    "# Tips:\n",
    "# - Run individual cells with the Run Cell button.\n",
    "# - Use the integrated terminal to run modules: `python -m src.vectorize` (if you add an entrypoint).\n",
    "# - Run tests from Test Explorer or via `pytest -q`.\n",
    "# - To debug, open a `.py` file, set breakpoints, and Run > Start Debugging or use the debug pane.\n",
    "\n",
    "print('Notebook: environment & reproducible math derivations for TF-IDF drift detection')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
